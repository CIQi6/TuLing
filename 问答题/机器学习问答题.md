# 吴恩达深度学习篇

### 1.1 简述过拟合和欠拟合的原因

- 过拟合
  1. 模型参数过多，导致模型对训练集过分学习，而不是针对普遍数据特征的学习，类似于记答案，而不思考方法。
  2. 训练集数据过少，某些数据存在误差，导致了模型学习出错，从而出现对训练集预测表现好，对未知数据预测表现差

- 欠拟合
  1. 参数过少，模型过于简单，就像用一条直线将数据分成三类。

### 1.2 简述正则化的原理

在损失函数上加上一个惩罚项，让损失函数中的某些项趋近于0，从而让某些参数不起作用，达到减少参数的目的。

### 1.3 简述梯度下降的原理

利用导数找到损失函数下降的方向，不断迭代修正参数让损失函数的值达到最小。

### 1.4 简述梯度下降的优化方法

- 动量法——利用累加历史的梯度信息乘以一个系数，来更新梯度。利用动量法也可以解决局部最小值或鞍点

- 自适应梯度法（AdaGrad）——给不同变化程度的方向设置不同的学习率（步长），减少变化大的方向的学习率，增大变化慢的方向的学习率。
- 自适应梯度法（RMSProp）——对AdaGrad的改进，使用指数加权移动平均的方法计算累积梯度，AdaGrad是所有的历史的梯度信息都用，而RMSProp只利用一定范围内的
- Adam——动量法和自适应梯度法的结合，同时加上了一个修正偏差项，避免了冷启动问题。

### 1.5 简述常用的激活函数

- sigmod
- tanh
- Relu
- Leakly Relu

### 1.6 简述神经网络的前向传播和反向传播

- 前向传播，输入经过计算，激活函数输出，再作为下一层的输入经过计算，激活输出。
- 反向传播，是神经网络最终输出的结果（代价函数）对网络中每个参数的偏导，用来学习参数的。

### 1.7 简述常用的预防过拟合的方法

- L1、L2正则化——在损失函数加上L1或L2范数，达到参数惩罚的作用，实现正则化。
- Dropout——让隐层的神经元以一定比率不被激活（丢弃），降低模型的参数容量，从而达到预防过拟合的作用。又因为Dropout只在训练过程中，所以测试时要乘上这个失活比率，或者采用Inverted Dropout。

### 1.8 简述使用全连接层结构处理图像时会出现什么问题

- 参数过多，全连接层中每一层中的每一个神经元都和前一层的所有神经元相连。

### 1.9 简述卷积的类型，什么是卷积感受野

- 卷积、转置卷积
- 卷积感受野是卷积层输出的特征图上的一个像素对应输出时原图的区域。